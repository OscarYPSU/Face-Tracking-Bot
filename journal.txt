Current implementation consists of:

Face tracking and face locking using 2 MG servos for rotational and a esp32-s3 with OV5680 camera module to capture live video which is scraped in a python backend for facial reconition processing nd coordination analyst to send servo coordinates over to microcontroller so microcontroller knows where to move in terms of x,y axis

current implementation also has emotional recognition 

form of communication between python backend server and microntroler is a UART form utilizing serial and custom prcessed data.  

thge scraping of live streaming from esp32-s3 is done so by usiung CV2 library on a custom IP that only shows the live streaming . aka a :81 port

utilizes multi threading to handle continuas scpraing of live streaming and for facial recognition and sending UART data on the side 

emotional recognition is done so by using a FER library

Moved CameraWebServer files to a new directory. Refactored OLED display code to use a shared display object and modularized emoticon rendering (happy, angry, study faces) with new header and source files. Added multi-threaded emotion data sending in FacialRecognition.py and updated rotational sketch to support emotion-based display switching. Added a project journal for documentation.


Problem:
Your OLED display initialization (display.begin(...)) kept failing, printing “SSD1306 allocation failed” and entering the infinite loop. However, when you removed the studySetup() call, it suddenly worked fine.
Cause:
studySetup() was being called immediately after display.begin(), before the display was fully ready or before other hardware (like I²C devices or pins) finished initializing. Inside studySetup(), something was interfering with the I²C communication — possibly reinitializing the Wire bus, using the same pins, or drawing to the display before it was ready. That caused display.begin() to fail because the OLED never received the proper I²C handshake.
Solution:
You added a short delay after display.begin() (for example, delay(100);) to give the OLED time to stabilize before calling studySetup(). That allowed the display initialization to complete properly, so the program could proceed without hitting the allocation error.



Introduced pet stats (hunger, health, sleepiness, happiness, age) and their update intervals in CentreFace.ino. Updated study.cpp to use new animation frames and removed the old book cartoon bitmap. Modified FacialRecongnition.py to send emotions in lowercase. Added empty DB/insert.py and DB/retrieve.py for future database operations.


Problem/Conflict:
While developing a simple game on an Arduino with an OLED display, I wanted to animate a character using multiple bitmap frames. My first instinct was to use delay() for timing the frames. However, I quickly realized that using delay() blocked the rest of the code, meaning I couldn’t update the game state or read input while the animation was running. I also considered multi-threading to handle animations separately, but the Arduino Uno doesn’t support threads, so that approach wasn’t possible.
I needed a solution that would allow the animation to run smoothly while still allowing other tasks like checking player input, updating scores, and handling collisions.
Solution:
I implemented a non-blocking timer approach using millis(). I kept track of the last frame update and compared it with the current time. Whenever enough time had passed, I updated the animation frame and reset the timer. This allowed the loop to continue running other tasks in parallel with the animation, essentially simulating multitasking without real threads.
Result/Outcome:
The character animation ran smoothly at the desired frame rate, and the game could still respond to input, update the state, and handle other logic simultaneously. It taught me a valuable lesson about working within the constraints of microcontrollers and designing code to be efficient and non-blocking, even without advanced features like threading.

Problem:
While working on a microcontroller project, I needed to share a timing variable (unsigned long millis) across multiple .cpp files. Initially, I defined the variable directly in the header file. This caused linker errors because each file that included the header created its own separate definition, violating the One Definition Rule in C++.
Solution:
I learned that global variables should only be declared in headers and defined in a single .cpp file. To fix the issue, I used the extern keyword in the header to declare the variable and initialized it only once in the main .cpp file. This ensured all modules referenced the same variable, eliminating 


Introduced neutralFace.cpp and neutralFace.h for a new emoticon. Added milis.h for millis tracking. Removed unnecessary display includes from headers, deleted display.cpp, and updated CentreFace.ino to set up the neutral face on startup. duplicate symbol errors.

Consolidated multiple face emotion implementations into a single 'emotions' module, removing redundant face files and headers. Added servo control abstraction and refactored the main Arduino sketch to use new emotion and servo modules. Introduced a Flask-based web interface with a simple pet logic backend, including real-time hunger display, and added serial communication scripts. Cleaned up unused code and improved modularity for easier maintenance and extension. (wrote HTTPS endpoints for getting data) Used asyncronsouse functrion in javacript in html for continuously update in hunger display using fetch

Refactored pet logic into a Pet class with thread-safe stat updates and new API endpoints for hunger and happiness. Updated Serial.py to add emotion detection and face tracking, sending data to a microcontroller. Improved the web interface to support feeding and display happiness, and updated Flask routes for RESTful API design.

Introduced new API endpoints and logic for sleepiness and play actions in the virtual pet application. Updated the Pet class to handle sleep and play states, including periodic stat updates based on pet status. Enhanced the frontend to display and interact with sleepiness and happiness, allowing users to play with or put the pet to sleep via new buttons and asynchronous JavaScript functions.

Moved and renamed logic.py to pet.py, and updated Main.py to use the new PetLogic modules. Added petFood.py with a basic Food class and Cupcake subclass. Moved static and template files to new locations, splitting JavaScript into static/home.js and updating home.html to reference it. Added redisTest.py for Redis and PostgreSQL integration testing. Minor improvements to Serial.py and pet logic, including a ToggleSleep method.

Developed a secure web application using Flask, Flask-Login, and Redis for session management.

Implemented user authentication and registration with PostgreSQL as the backend database, including INSERT and SELECT queries for storing and verifying credentials.

Integrated Redis to manage user sessions in a scalable and efficient manner, enabling session persistence across server restarts.

Designed a login workflow with session-based access control, protecting routes using @login_required decorators.

Built dynamic templates using render_template_string to handle login and dashboard views.

Applied secure coding practices by using parameterized queries (%s) to prevent SQL injection attacks.

Managed database connections and cursors responsibly to ensure data integrity.

Technologies used: Python, Flask, Flask-Login, Flask-Session, Redis, PostgreSQL (psycopg2), HTML forms.

Developed a real-time digital pet simulation using Flask and Flask-Login for user session management.

Designed RESTful API endpoints (GET, POST, PUT) to interact with the pet’s state, including hunger, happiness, and sleepiness, enabling dynamic updates from the frontend.

Implemented backend game logic in a separate thread to continuously update pet attributes independently of web requests.

Used JSON responses to provide a clean interface between backend logic and frontend templates.

Created interactive routes for feeding, playing, and sleeping, demonstrating event-driven state changes.

Integrated Python classes and object-oriented design (Pet and PetFood) to manage game mechanics modularly.

Technologies used: Python, Flask, Flask-Login, threading, RESTful APIs, JSON, HTML templates.

Developed a real-time face tracking and emotion detection system using OpenCV and FER (Facial Emotion Recognition).

Captured live video streams from an ESP32 camera and processed frames to detect faces and extract top emotions.

Designed multi-threaded Python architecture to simultaneously:

Track face positions and send coordinates to a microcontroller via serial communication.

Detect emotions and transmit emotion data to an OLED display connected to the microcontroller.

Applied Haar Cascade classifiers for fast face detection and optimized frame processing for real-time performance.

Implemented data encoding and transmission over serial ports to interface with embedded devices, enabling responsive interaction between software and hardware.

Added debugging and visualization features using OpenCV to display detected faces, emotion labels, and tracking coordinates.

Technologies used: Python, OpenCV, FER, threading, serial communication, ESP32 camera, real-time data streaming.